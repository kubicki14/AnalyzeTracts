{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early game running start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source - https://medium.com/@rqaiserr/how-to-convert-pdfs-into-searchable-key-words-with-python-85aab86c544f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import requests\n",
    "import nltk\n",
    "import PyPDF2 \n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('punkt') # necessary jaxzz to tokenize\n",
    "# for downloading pdf's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import raw dataset from kaggle\n",
    "data = pd.read_csv('../raw/contracts.csv', encoding='utf-8')\n",
    "# Change column headers to utilize underlines instead of whitespace.\n",
    "data.columns = data.columns.str.replace(' ', '_')\n",
    "# Use list comprehension to make all lowercase headers as well.\n",
    "data.columns = [x.lower() for x in data.columns]\n",
    "# Output first 5 rows (head)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows? (before filter)\n",
    "len(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and create a df with only contracts that have an attached url\n",
    "data['contract_pdf'].replace('', np.nan, inplace=True)\n",
    "data.dropna(subset=['contract_pdf'], inplace=True)\n",
    "# How many rows? (after filter should  be 45,775)\n",
    "len(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See where were at again...\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use regex to filter and change column to have JUST the url itself:\n",
    "# Just grab group 1\n",
    "remove_junk = re.compile(r\"{'url':\\s'(.*)'}\", re.IGNORECASE)\n",
    "data['contract_pdf'] = data['contract_pdf'].apply(lambda x: re.search(remove_junk, x).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point we'll create a function that takes a row, downloads the pdf, extracts the text, and stores the pdf.\n",
    "# ONLY PERFORMING ON ONE PDF RIGHT NOW! Make for-loop later...\n",
    "    \n",
    "def download_pdf(row):\n",
    "    url=row['contract_pdf']\n",
    "    response = requests.get(url) # We'll grab the response.text (html output of page), grab REAL pdf link and download.\n",
    "    # Use regex to pull the link out....Let's hope all chicago contracts follow same html format ;)\n",
    "    pdf_regex = re.compile(r'<iframe src=\"(.*)\"\\sname=', re.IGNORECASE) # find link; this regex should be standard on all state pages.\n",
    "    new_link = re.search(pdf_regex, response.text).group(1) # REAL pdf link\n",
    "    r = requests.get(new_link, allow_redirects=True, stream=True)\n",
    "    with open('../raw/chicago_pdfs/' + str(row['specification_number']) + '-' + str(row['vendor_id']) + '.pdf', 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "# Download pdf for each row\n",
    "data.apply(download_pdf, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# VERIFIED EXTRACTION CODE TO DOWNLOAD PDF FROM STATE SITES (pain in the butt, took me way too long...)\n",
    "\n",
    "# url = 'http://ecm.cityofchicago.org/eSMARTContracts/service/DPSWebDocumentViewer?sid=ESMART&id={2488393F-CCF9-476E-808A-9FBF3C25E0D6}'\n",
    "# response = requests.get(url) # We'll grab the response.text (html output of page), grab real pdf link and download.\n",
    "# # Use regex to pull the link out....Let's hope all chicago contracts follow same html format ;)\n",
    "# pdf_regex = re.compile(r'<iframe src=\"(.*)\"\\sname=', re.IGNORECASE) # find link; this regex should be standard on all state pages.\n",
    "# new_link = re.search(pdf_regex, response.text).group(1)\n",
    "# r = requests.get(new_link, allow_redirects=True, stream=True)\n",
    "# with open('test.pdf', 'wb') as f:\n",
    "#     f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crazy method because pdf's embedded in js in the redirected page....ffs\n",
    "import BeautifulSoup\n",
    "url = 'http://ecm.cityofchicago.org/eSMARTContracts/service/DPSWebDocumentViewer?sid=ESMART&id={2488393F-CCF9-476E-808A-9FBF3C25E0D6}'\n",
    "#for iframe in iframexx:\n",
    "    #response = urllib2.urlopen(iframe.attrs['src'])\n",
    "    #iframe_soup = BeautifulSoup(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def internet_pdf_to_text(row): \n",
    "    # stuff here\n",
    "        \n",
    "# For each row take text from downloaded pdf associated file/delete it?\n",
    "data['text_list'] = data.apply(internet_pdf_to_text, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ___________ENDOFDAY_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ PDF INTO TEXT STEP #####\n",
    "###############################\n",
    "#write a for-loop to open many files -- leave a comment if you'd #like to learn how\n",
    "\n",
    "filename = '../raw/chicago_pdfs/example_handwriting_B29611004.pdf'\n",
    "\n",
    "#open allows you to read the file\n",
    "\n",
    "pdfFileObj = open(filename,'rb')\n",
    "\n",
    "#The pdfReader variable is a readable object that will be parsed\n",
    "\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "#discerning the number of pages will allow us to parse through all #the pages\n",
    "\n",
    "num_pages = pdfReader.numPages\n",
    "count = 0\n",
    "text = \"\"\n",
    "\n",
    "#The while loop will read each page\n",
    "while count < num_pages:\n",
    "    pageObj = pdfReader.getPage(count)\n",
    "    count +=1\n",
    "    text += pageObj.extractText()\n",
    "\n",
    "#This if statement exists to check if the above library returned #words. It's done because PyPDF2 cannot read scanned files.\n",
    "\n",
    "if text != \"\":\n",
    "   text = text\n",
    "\n",
    "#If the above returns as False, we run the OCR library textract to #convert scanned/image based PDF files into text\n",
    "\n",
    "else:\n",
    "   text = textract.process(fileurl, method='tesseract', language='eng')\n",
    "\n",
    "# Now we have a text variable which contains all the text derived #from our PDF file. Type print(text) to see what it contains. It #likely contains a lot of spaces, possibly junk such as '\\n' etc.\n",
    "\n",
    "# Now, we will clean our text variable, and return it as a list of keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The word_tokenize() function will break our text phrases into #individual words\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "#we'll create a new list which contains punctuation we wish to clean\n",
    "punctuations = ['(',')',';',':','[',']',',']\n",
    "\n",
    "#We initialize the stopwords variable which is a list of words like #\"The\", \"I\", \"and\", etc. that don't hold much value as keywords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#We create a list comprehension which only returns a list of words #that are NOT IN stop_words and NOT IN punctuations.\n",
    "\n",
    "keywords = [word for word in tokens if not word in stop_words and not word in punctuations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords # This  is the list of all parsed information, which is attempted to be parsed further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First objective: Create column that contains text from pdf->text (tesseract?)\n",
    "# (Or better option than tesseract for python if exists...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second objective: Make a column utilizing web scraping on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third objective: Create truth column that deduces whether or not a contract\n",
    "# was ACCEPTED/DENIED utiilizing regex and looking at column created above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth objective: Create a list of prioritized vendors to imititate based off:\n",
    "# -How many contracts they've acquired\n",
    "# -Value of contracts (ie: give higher weight/importance of imitation to\n",
    "    # high-paying contract obtainers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once curated dataset created, upload to Kaggle as a kernel and call TIDY_contract_data.csv/xlsx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Late game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text classifier that finds and verifies a piece ofo text exists in\n",
    "# all contracts (ie: a standard or substandard that is followed but sometimes  forgotten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR create a feature bayesian inference to discern acceptance or denial of contract"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
