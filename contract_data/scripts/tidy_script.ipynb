{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early game running start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source - https://medium.com/@rqaiserr/how-to-convert-pdfs-into-searchable-key-words-with-python-85aab86c544f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import requests\n",
    "import nltk\n",
    "import PyPDF2 \n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('punkt') # necessary jaxzz to tokenize\n",
    "# for downloading pdf's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Clean Main dataset and Download State PDF Contracts Remote->Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import raw dataset from kaggle\n",
    "data = pd.read_csv('../raw/contracts.csv', encoding='utf-8')\n",
    "# Change column headers to utilize underlines instead of whitespace.\n",
    "data.columns = data.columns.str.replace(' ', '_')\n",
    "# Use list comprehension to make all headers lowercase as well.\n",
    "data.columns = [x.lower() for x in data.columns]\n",
    "# Change contract_url header to say just that...\n",
    "data.rename(columns={'contract_pdf': 'contract_url'}, inplace=True)\n",
    "# Output first 5 rows (head)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows? (before filter)\n",
    "len(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and create a df with only contracts that have an attached url\n",
    "data['contract_url'].replace('', np.nan, inplace=True)\n",
    "data.dropna(subset=['contract_url'], inplace=True)\n",
    "# How many rows? (after filter should  be 45,775)\n",
    "len(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See where were at again...\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use regex to filter and change column to have JUST the url itself:\n",
    "# Just grab group 1\n",
    "remove_junk = re.compile(r\"{'url':\\s'(.*)'}\", re.IGNORECASE)\n",
    "data['contract_url'] = data['contract_url'].apply(lambda x: re.search(remove_junk, x).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point for each row we'll download the pdf and store the pdf locally.\n",
    "    \n",
    "def download_pdf(row):\n",
    "    url=row['contract_url']\n",
    "    response = requests.get(url) # We'll grab the response.text (html output of page), grab REAL pdf link and download.\n",
    "    # Use regex to pull the link out....Let's hope all chicago contracts follow same html format ;)\n",
    "    pdf_regex = re.compile(r'<iframe src=\"(.*)\"\\sname=', re.IGNORECASE) # find link; this regex should be standard on all state pages.\n",
    "    new_link = re.search(pdf_regex, response.text).group(1) # REAL pdf link\n",
    "    r = requests.get(new_link, allow_redirects=True, stream=True)\n",
    "    with open('../raw/chicago_pdfs/' + str(row['specification_number']) + '-' + str(row['vendor_id']) + '.pdf', 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "# Download pdf for each row using its contract_url\n",
    "data.apply(download_pdf, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# VERIFIED EXTRACTION CODE TO DOWNLOAD PDF FROM STATE SITES (pain in the butt, took me way too long...)\n",
    "\n",
    "# url = 'http://ecm.cityofchicago.org/eSMARTContracts/service/DPSWebDocumentViewer?sid=ESMART&id={2488393F-CCF9-476E-808A-9FBF3C25E0D6}'\n",
    "# response = requests.get(url) # We'll grab the response.text (html output of page), grab real pdf link and download.\n",
    "# # Use regex to pull the link out....Let's hope all chicago contracts follow same html format ;)\n",
    "# pdf_regex = re.compile(r'<iframe src=\"(.*)\"\\sname=', re.IGNORECASE) # find link; this regex should be standard on all state pages.\n",
    "# new_link = re.search(pdf_regex, response.text).group(1)\n",
    "# r = requests.get(new_link, allow_redirects=True, stream=True)\n",
    "# with open('test.pdf', 'wb') as f:\n",
    "#     f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This  function says for each row we have an associated pddf, extract text from it. (see what happens!)\n",
    "def pdf_to_text(row): \n",
    "    # Grab what the filename should be\n",
    "    filename = '../raw/chicago_pdfs/' + str(row['specification_number']) + '-' + str(row['vendor_id']) + '.pdf'\n",
    "        \n",
    "# For each row take text from downloaded pdf associated file/delete it?\n",
    "data['text_list'] = data.apply(pdf_to_text, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Local_PDF -> List of strings representing OCR output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE: Was having a bad time getting this to work on small_example.pdf, try this tomorrow \n",
    "# https://pythontips.com/2016/02/25/ocr-on-pdf-files-using-python/\n",
    "\n",
    "# PDF->TEXT (sample/test)\n",
    "filename = '../raw/chicago_pdfs/small_example.pdf'\n",
    "#open allows you to read the file\n",
    "# pdfFileObj = open(filename,'rb')\n",
    "# #The pdfReader variable is a readable object that will be parsed\n",
    "# pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "# #discerning the number of pages will allow us to parse through all #the pages\n",
    "# num_pages = pdfReader.numPages\n",
    "# count = 0\n",
    "text = \"\"\n",
    "# #The while loop will read each page\n",
    "# while count < num_pages:\n",
    "#     pageObj = pdfReader.getPage(count)\n",
    "#     count +=1\n",
    "#     text += pageObj.extractText()\n",
    "# Check if PyPDF can acqire the text easily. If not it's most likely a scanned image put in a pdf file...This has potential problems...the logic that is.\n",
    "if text == '':\n",
    "    # If here  means our PyPDF extract failed, and we need to do something more advanced....tesseract!\n",
    "    text = textract.process(filename)#, method='tesseract', encoding='ascii', language='eng') #method='pdfminer', language='eng')#\n",
    "\n",
    "# Now we have a text variable which contains all the text derived #from our PDF file. Type print(text) to see what it contains. It #likely contains a lot of spaces, possibly junk such as '\\n' etc.\n",
    "print(text)\n",
    "# Now, we will clean our text variable, and return it as a list of keywords.\n",
    "#----------------SEPARATE PARSED INTO LIST\n",
    "#The word_tokenize() function will break our text phrases into #individual words\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "#we'll create a new list which contains punctuation we wish to clean\n",
    "punctuations = ['(',')',';',':','[',']',',']\n",
    "\n",
    "#We initialize the stopwords variable which is a list of words like #\"The\", \"I\", \"and\", etc. that don't hold much value as keywords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#We create a list comprehension which only returns a list of words #that are NOT IN stop_words and NOT IN punctuations.\n",
    "keywords = [word for word in tokens if not word in stop_words and not word in punctuations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords # This  is the list of all parsed information, which is attempted to be parsed further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pythontips.com/2016/02/25/ocr-on-pdf-files-using-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First objective: Create column that contains text from pdf->text (tesseract?)\n",
    "# (Or better option than tesseract for python if exists...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Late game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second objective: Make a column utilizing web scraping on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third objective: Create truth column that deduces whether or not a contract\n",
    "# was ACCEPTED/DENIED utiilizing regex and looking at column created above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth objective: Create a list of prioritized vendors to imititate based off:\n",
    "# -How many contracts they've acquired\n",
    "# -Value of contracts (ie: give higher weight/importance of imitation to\n",
    "    # high-paying contract obtainers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once curated dataset created, upload to Kaggle as a kernel and call TIDY_contract_data.csv/xlsx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text classifier that finds and verifies a piece ofo text exists in\n",
    "# all contracts (ie: a standard or substandard that is followed but sometimes  forgotten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR create a feature bayesian inference to discern acceptance or denial of contract"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
